\section{Methodology}\label{sec:methodology}
The \ac{db2} has \var{entries_db_all} independent recordings from various field investigations throughout the world, and it is the largest of its kind~\cite{FoldvaryLicina2018, db2dryad}.
\Ac{tsv} were collected using a right-here-right-now survey and environmental parameters were measured and logged in the proximity of participants when they completed the survey.
The \ac{db2} only contains data collected by researchers in cross-sectional field studies that have been published in peer-reviewed journals.
 We compared the results obtained by the \ac{pmv} and the \ac{pmv-ce} with the self-reported \ac{tsv}.
Results were calculated using the function `pythermalcomfort.models.pmv\_ppd' included in pythermalcomfort v2.8.1 a Python package for thermal comfort calculations~\cite{Tartarini2020a}.

\subsection{Data Preparation and Cleaning}\label{subsec:data-processing-and-cleaning}
Not all researchers measured and logged all six parameters needed to calculate the \ac{pmv} and \ac{pmv-ce}.
We, therefore, decided to remove all entries that did not have any of the following variables: \ac{tdb}, \ac{rh}, \ac{v}, \ac{clo}, \ac{met}, and \ac{tsv}.
This filtered out approximately \var{entries_db_valid} of the data.
We decided to keep entries that did not have \ac{tr}.
However, since \ac{tr} is a required input in the \ac{pmv} model, it was calculated using the following equation: $2 t_{o} â€“ t_{db}$.
If the \ac{to} was not available for that entry we assumed \ac{tr} = \ac{tdb}.
We are aware that these assumptions may introduce an error, but removing samples without \ac{tr} would cause an additional \var{entries_db_valid_no_tr} drop in the data used to compare the models.
There is also evidence that in many conditions is possible to assume \ac{tr} equal to \ac{tdb} without committing a major error~\cite{Dawe2020}.

Both the \gls{55} and \gls{7730} specify a set of applicability limits, we hence filtered out those data points that did not meet the inclusion criteria of both Standards.
The rationale is that the models' accuracy should only be tested within their applicability ranges.
The limits we used are as follows:
\num{10}~$\leq$~\ac{tdb}~$\leq$~\qty{30}{\celsius},
\num{10}~$\leq$~\ac{tr}~$\leq$~\qty{40}{\celsius},
\num{0}~$\leq$~\ac{v}~$\leq$~\qty{1}{\m\per\s},
\num{0}~$\leq$~\ac{clo}~$\leq$~\qty{1.5}{clo},
\num{0}~$\leq$~water vapour partial pressure~$\leq$~\qty{2000}{\pascal},
and \num{1}~$\leq$~\ac{met}~$\leq$~\qty{4}{met}.
We then calculated the adjusted total clothing insulation and relative airspeed as required by both Standards.
We used these inputs to calculate the \ac{pmv} and \ac{pmv-ce} values.

Fanger and the \gls{7730} state that the \ac{pmv} should only be used when its absolute value is lower than 2~\cite{Fanger1970, iso7730}.
However, since the thermal sensation was measured with at least a seven-point scale and the \ac{pmv} has no upper or lower boundary we decided to keep the data that felt within the following ranges $|$\ac{tsv}$|$~$\leq$~\num{3} or $|$\ac{pmv}$|$~$\leq$~\num{3.5}.

In addition, it should also be noted that while all the data included in the \ac{db2} is published in peer-reviewed papers, not all entries were collected using the same methodology.
For example, while both thermal comfort Standards recommend measuring \ac{v} at three different heights, this was not always done.
This could introduce a bias in the data.
% todo this is not true since it also depends on the metabolic rate of the occupants, we should look at vr and not v
Since the two \ac{pmv} formulations only differ when \ac{v}~$\geq$~\qty{0.1}{\m\per\s}, we decided to report three sets of results:
\begin{enumerate}
    \item all the data points,
    \item all the data points with \ac{v}~$\geq$~\qty{0.2}{\m\per\s},
    \item only the data points with \ac{v}~$\geq$~\qty{0.2}{\m\per\s} and from those studies that measured \ac{v} at three different heights.
    These results are reported in the supplementary material.
\end{enumerate}
We believe that this approach will help us understand the impact of the data quality on the results of the models.
In particular, by only including those studies that measured \ac{v} at three different heights, we can reduce the bias introduced by the lack of adherence to the measurement protocol recommended by the Standards.

We used Python to analyse and visualise the results.
We are committed to reproducible research hence we have shared the source code and the dataset we used publicly at this URL: \url{https://github.com/FedericoTartarini/paper-pmv-comparison} so other users can test different assumptions.

\subsection{Model Validation}\label{subsec:model-validation}
The \ac{pmv} model was developed to predict the average thermal sensation of a large group of occupants, but it is commonly used both in singly occupied zones and in areas with several hundred people.
We first grouped participants' responses by their \ac{tsv}, and we determined how many of these votes were correctly labelled by the \ac{pmv} and \ac{pmv-ce} models.
This approach is useful for performing an overall assessment of the accuracy of the models, but it introduces an error due to the rounding of both the \ac{pmv}, \ac{pmv-ce}, and \ac{tsv} values.
The latter were sometimes collected using a continuous scale and not a discrete one, in other words, they could take any value in the range from -3.0 to 3.0.

To remove this rounding error, we subtracted the \ac{tsv} value from the \ac{pmv} and \ac{pmv-ce} values.
These differences, also known as bias, quantify the success of the model in predicting \ac{tsv}.
However, on their own are a low-precision estimate of the overall accuracy of the model~\cite{Humphreys2002}.
These values were then binned using several independent variables (e.g., \ac{tdb}, \ac{v}).
If the \ac{pmv} or \ac{pmv-ce} formulations are bias-free, the distribution of any batch derived from these differences would have a mean value that is zero.
The standard deviation would reflect the combined effect of the people's individual differences as well as any errors in formulation or in the data collection method (accuracy or precision of the instrumentation used)~\cite{Humphreys2002}.
According to \mycite{Humphreys2002} the model can be considered accurate if the above-mentioned differences are between \num{-0.25} and \num{0.25}.
However, we believe that this range is too narrow since \ac{tsv} is measured using an ordinal scale of integers.
Consequently, we expanded the range to \num{-.5} and \num{0.5}.
This is needed to ensure that each \ac{pmv} value can be matched to a \ac{tsv}.
We are aware that an error of \num{.5} is approximately \qty{7}{\percent} of the total range and ideally, we would want to have the \ac{pmv} to have higher precision, however, this is a necessary limitation introduced by the low resolution of the \ac{tsv} scale.
Binning data by the independent variables is a significant improvement over grouping the participants' responses by their \ac{tsv}.
This is because the \ac{pmv} model aim is, as previously mentioned, to predict the average thermal sensation of a large group of occupants and not to correctly predict each individual \ac{tsv} of people who reported the same \ac{tsv} vote and where in different locations and moments of the year and day.

%In \ref{sec:comparison-between-the-pmv-formulations-with-the-two-node-model} we also compare the results of \ac{pmv} and \ac{pmv-ce} formulations with the \ac{pmvg} calculated by the two-node model.
%The two-node model was developed by \mycite{GaggeSET} and it uses a more accurate set of equations to solve the heat balance equation.
%We compared the \ac{pmv} and \ac{pmv-ce} results to the \ac{pmvg} since Gagge's two-node model is used by the \ac{pmv-ce} to calculate the \ac{ce}.
%Hence, in principle, the results of the \ac{pmv-ce} model should closely match the \ac{pmvg} values.
%We did this by passing as input to the \ac{pmv}, \ac{pmv-ce}, and two-node models the same set of \num{10000} randomly generated set of inputs.
%Each set of inputs was sampled assuming a random uniform distribution within a predefined range.
%We selected the range of each of the 6 input variables needed to calculate the \ac{pmv} to be equal to the 2.5th and 97.5th percentiles for all the input variables contained in the \ac{db2}.
%The only exception was for the range of \ac{v} that was randomly selected from this interval \qty{0.1}{\m\per\s}~$\leq$~\ac{v}~$\leq$~\qty{1}{\m\per\s}.
%The rationale for this decision was that both the \ac{pmv} and \ac{pmv-ce} models use the same equations for \ac{v}~$\leq$~\qty{0.1}{\m\per\s}.
%
%Interestingly, \mycite{GaggeSET} also describe how to calculate the \ac{pmvs} which uses the \ac{set} temperature to calculate the \ac{pmv} value in place of \ac{tdb} and \ac{tr}.
%This would solve some of the uncertainties that the \ac{pmv-ce} model introduces as detailed above.
%However, it is not possible for us to comment on why the \ac{pmv-ce} model was included in the \gls{55} instead of the \ac{pmvs} since no peer-reviewed paper or official document describes the rationale for this decision.

\subsection{Performance Metrics}\label{subsec:performance-metrics}
Simple accuracy can be a misleading metric to use to quantify the performance of a classification predictive model that involves an unbalanced dataset~\cite{Chawla2005}.
We then also report the F1 score when determining the performance of the \ac{pmv} and \ac{pmv-ce} in predicting the \ac{tsv}.
The F1 score is the harmonic mean of the precision and sensitivity, where an F1 score reaches its best value at 1 and the worst score at 0.
Precision (also called positive predictive value) is the ratio of true positive overall positive results obtained from the test, while sensitivity (also known as recall or true positive rate) is the ratio of true positive over all the actual positive cases.
We report three F1 scores in the paper.
The F1-micro score calculates metrics globally by counting the total true positives, false negatives, and false positives.
The F1-macro calculate metrics for each label and find their unweighted mean.
The F1-weighted calculate metrics for each label and find their average weighted by support (the number of true instances for each label).

% Ed's comments below
% The primary sticking point is with the input data.  The following issue seems fundamental to me, something you would have to address in a published paper about this. 

% Air speed measurements in the ASHRAE Field Dataset rarely conform to ASHRAE or ISO standards.  Only the studies done with instrumented carts with anemometer arrays are likely to have measured at three heights as required.   We know that very few of the studies have done this.
 
% If you only measure at one height instead of three, you are very likely to record higher values than the averaged velocities that are required for the standardsâ€™ comfort models.   This is because the anemometer is almost always placed in relatively open space between mid-body and head level, avoiding measuring the usually sheltered lower body level.  The measurement will therefore most often be an overestimate for inputting into the model, and the associated sensation prediction will be cooler than it should be.  
 
% So if you have a systematic bias toward higher airspeeds in the statistical data you are using, a comfort model that inherently predicts stronger rates of air speed cooling (CE) will overpredict more than a model that inherently predicts weaker rates (PMV).  The weaker one will appear to be predicting correctly, or in any event something closer to correct.  The stronger-predicting model could appear â€˜biasedâ€™ when compared to the weaker-predicting model.  
 
% This is one of the points of Pearlâ€™s causal inference bookâ€”in assigning bias, you have to find the source of the bias, which in this case may be in the dataset, and not in either of the models being evaluated.  
 
% The DB2 dataset is very useful for temperature, but it is really not suitable for detailed evaluation of air speed.  Air speed is far more variable and harder to measure accurately than temperature.  I suppose someone could wade through all the studies and observations in the database and observe the types of conditions under which the elevated air speeds were recorded.  Were they consistent over time and geographic location? Did they average over sufficient time as specified in standards?   Any sense of the accuracy of the anemometers used to measure them?  What kind of buildings were they inâ€”or any indications about the thermal sensitivity of the occupancies being surveyed?  You can see that all these questions suggest possible sources of bias, all which would be need to identified and assessed, to judge whether information from such surveys is suitable for evaluating a systematic measurement system like a comfort model.  It seems a really big task with uncertain benefits.
 
% Models vs laboratory study data:
 
% In our 2015 Corrective Power paper, Hui assembled the results from many laboratory studies of cooling devices, including fans.   In this paper you can see the studies of Tanabe where his subjects were exposed to a uniform plug flow, where the air movement measurement represents the whole body as required by the standard.   You can compare his results to the CE model and find that the CE model slightly underpredicts the subjective cooling votes he measured.  
 
% If you then look at the cooling found by Yongchao Zhai where the air movement was directed only at the upper body and measured there, the CE model slightly overpredicts his measured results.   This is of course appropriate.   It is seen in other recent studies of local fan cooling.  
 
% Bigger view of this:

% I think the comparison of models must be done using data from controlled experiments in the laboratory.  A lot of this was well-examined 50 years ago.  It hasnâ€™t been followed up as intensely since then because it wasnâ€™t considered that important by the profession.  Whatâ€™s different now is that 1) people are recognizing climate warming, and 2) itâ€™s a lot easier to do comfort and physiological modeling and learn from it, using the Web.    Thatâ€™s why there is now an opportunity that people can design better buildings using more sophisticated comfort and climate information. 